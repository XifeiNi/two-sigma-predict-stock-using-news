\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{General Word Embedding Models}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{First Attempt: Hot One Encoding}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Second Attempt: Bag of Words \(BoW\) Model}{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{Third Attempt: Neural Network Based \(NNLM\)}{section.2}% 5
\BOOKMARK [3][-]{subsubsection.2.3.1}{Autoencoder \(GAN\)}{subsection.2.3}% 6
\BOOKMARK [3][-]{subsubsection.2.3.2}{Neural Network Language Model \(NNLM\)}{subsection.2.3}% 7
\BOOKMARK [2][-]{subsection.2.4}{Fourth Attempt: Word2Vec}{section.2}% 8
\BOOKMARK [3][-]{subsubsection.2.4.1}{Continuous Bag-of-Word \(CBOW\)}{subsection.2.4}% 9
\BOOKMARK [3][-]{subsubsection.2.4.2}{Continuous Skip-gram\(Skip Gram\)}{subsection.2.4}% 10
\BOOKMARK [3][-]{subsubsection.2.4.3}{Refinement of Word2Vec}{subsection.2.4}% 11
\BOOKMARK [3][-]{subsubsection.2.4.4}{Results of Word2Vec}{subsection.2.4}% 12
\BOOKMARK [2][-]{subsection.2.5}{Global Vectors for Word Representation \(GloVe\)}{section.2}% 13
\BOOKMARK [2][-]{subsection.2.6}{Embedding from Language Model \(ELMo\)}{section.2}% 14
\BOOKMARK [2][-]{subsection.2.7}{Bidirectional Encoder Representation from Transformer \(BERT\)}{section.2}% 15
\BOOKMARK [3][-]{subsubsection.2.7.1}{Transformer}{subsection.2.7}% 16
\BOOKMARK [3][-]{subsubsection.2.7.2}{Masked Language Model}{subsection.2.7}% 17
\BOOKMARK [1][-]{section.3}{Question Answering}{}% 18
