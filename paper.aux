\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {}General Word Embedding Models}{1}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A result returned by Google to answer "Who is the first Prime Minister of Australia?"\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:google}{{1}{2}{A result returned by Google to answer "Who is the first Prime Minister of Australia?"\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}First Attempt: Hot One Encoding}{2}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces One Hot Encoding Example.\relax }}{3}{figure.caption.2}}
\newlabel{fig:vector}{{2}{3}{One Hot Encoding Example.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Bag of Words Vector Space Representation\relax }}{4}{figure.caption.3}}
\newlabel{fig:bow}{{3}{4}{Bag of Words Vector Space Representation\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Second Attempt: Bag of Words (BoW) Model}{4}{subsection.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table 1: Represent Words in Fewer Dimensions\relax }}{5}{table.caption.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Third Attempt: Neural Network Based (NNLM)}{5}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Autoencoder (GAN)}{5}{subsubsection.2.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Figure 4: Neural Network Based Mapping\relax }}{6}{figure.caption.5}}
\newlabel{fig:nnlm}{{4}{6}{Figure 4: Neural Network Based Mapping\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Basic Architecture of Autoencoder}{6}{section*.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Schematic structure of an autoencoder with 3 fully connected hidden layers. The code (z, or h for reference in the text) is the most internal layer.\relax }}{7}{figure.caption.6}}
\newlabel{fig:autoencoder}{{5}{7}{Schematic structure of an autoencoder with 3 fully connected hidden layers. The code (z, or h for reference in the text) is the most internal layer.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Neural Network Language Model (NNLM)}{9}{subsubsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Neural Architecture\relax }}{10}{figure.caption.8}}
\newlabel{fig:nnlm}{{6}{10}{Neural Architecture\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Fourth Attempt: Word2Vec}{11}{subsection.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Continuous Bag-of-Words\relax }}{12}{figure.caption.9}}
\newlabel{fig:cbow}{{7}{12}{Continuous Bag-of-Words\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Continuous Bag-of-Word (CBOW)}{12}{subsubsection.2.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Continuous Skip-gram(Skip Gram)}{12}{subsubsection.2.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Continuous Skip-Gram\relax }}{13}{figure.caption.10}}
\newlabel{fig:skip-gram}{{8}{13}{Continuous Skip-Gram\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Refinement of Word2Vec}{13}{subsubsection.2.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Hierarchical Softmax Model\relax }}{14}{figure.caption.12}}
\newlabel{fig:skip-gram}{{9}{14}{Hierarchical Softmax Model\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Hierarchical Softmax}{14}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Vector Space of Word2Vec\relax }}{15}{figure.caption.14}}
\newlabel{fig:skip-gram}{{10}{15}{Vector Space of Word2Vec\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline Negative Sampling}{15}{section*.13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Results of Word2Vec}{15}{subsubsection.2.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Two-layer Bidirectional LSTM Backbone\relax }}{16}{figure.caption.15}}
\newlabel{fig:skip-gram}{{11}{16}{Two-layer Bidirectional LSTM Backbone\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Global Vectors for Word Representation (GloVe)}{16}{subsection.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Embedding from Language Model (ELMo)}{16}{subsection.2.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Overview of the procedure on training ELMo\relax }}{18}{figure.caption.16}}
\newlabel{fig:skip-gram}{{12}{18}{Overview of the procedure on training ELMo\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Bidirectional Encoder Representation from Transformer (BERT)}{18}{subsection.2.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A graphic elaboration on ELMo function\relax }}{19}{figure.caption.17}}
\newlabel{fig:skip-gram}{{13}{19}{A graphic elaboration on ELMo function\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Transformer}{19}{subsubsection.2.7.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Input/Output Representation}{19}{subsubsection.2.7.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {}Masked Language Model}{19}{subsubsection.2.7.3}}
\@writefile{toc}{\contentsline {section}{\numberline {}Question Answering}{19}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\relax }}{20}{figure.caption.18}}
\newlabel{fig:skip-gram}{{14}{20}{BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.\relax }{figure.caption.18}{}}
\bibcite{Lectures}{1}
\bibcite{Workfunctions}{2}
